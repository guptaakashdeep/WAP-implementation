{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAP Implementation with Iceberg version > 1.2\n",
    "\n",
    "- After Iceberg version > 1.2, it supports `branching` and `tagging` snapshots which makes it easier to implement WAP framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/venv/lib/python3.8/site-packages/pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Iceberg jars \n",
    "- .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2')\n",
    "\n",
    "Loading Iceberg Extensions to call stored procedures\n",
    "- .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "\n",
    "Creating Iceberg catalog name of type `hive` that loads table from `Hive Metastore`. Adds support for spark built-in catalog \n",
    "- .config('spark.sql.catalog.spark_catalog','org.apache.iceberg.spark.SparkSessionCatalog')\n",
    "- .config('spark.sql.catalog.spark_catalog.type','hive')\n",
    "\n",
    "Creating Iceberg catalog named `local` of type `hadoop`. This supports directory based catalog in HDFS\n",
    "- .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog')\n",
    "- .config('spark.sql.catalog.local.type','hadoop')\n",
    "- .config('spark.sql.catalog.local.warehouse','<path_to_warehouse>') \\\n",
    "\n",
    "If `type` is `null`, `spark.sql.catalog.<catalog-name>.catalog-impl` **shouldn't** be `null`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "warehouse_directory = \"local_path\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"wap-iceberg-branching\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2')\\\n",
    "    .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.iceberg.spark.SparkSessionCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.type','hive') \\\n",
    "    .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.local.type','hadoop') \\\n",
    "    .config('spark.sql.catalog.local.warehouse',f'{warehouse_directory}/warehouse') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and creating an Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2846722"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading NYC Yellow Taxi Trip Data Sep 2023 data\n",
    "yellow_df = spark.read.parquet(\"../nyc-taxi-trips/yellow/sep-2023/\")\n",
    "yellow_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating month and year column\n",
    "from pyspark.sql.functions import lit\n",
    "yellow_df = yellow_df.withColumn(\"month\", lit(9)) \\\n",
    "        .withColumn(\"year\", lit(2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an Iceberg table in local catalog within nyc_tld database\n",
    "yellow_df.writeTo(\"local.nyc_tlc.yellow_taxi_trips\").partitionedBy(\"year\", \"month\").using(\"iceberg\").tableProperty(\"format-version\", \"2\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_stmt = spark.sql(\"show create table local.nyc_tlc.yellow_taxi_trips\").toPandas().to_dict(orient=\"records\")[0]['createtab_stmt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE local.nyc_tlc.yellow_taxi_trips (\n",
    "      VendorID INT,\n",
    "      tpep_pickup_datetime TIMESTAMP_NTZ,\n",
    "      tpep_dropoff_datetime TIMESTAMP_NTZ,\n",
    "      passenger_count BIGINT,\n",
    "      trip_distance DOUBLE,\n",
    "      RatecodeID BIGINT,\n",
    "      store_and_fwd_flag STRING,\n",
    "      PULocationID INT,\n",
    "      DOLocationID INT,\n",
    "      payment_type BIGINT,\n",
    "      fare_amount DOUBLE,\n",
    "      extra DOUBLE,\n",
    "      mta_tax DOUBLE,\n",
    "      tip_amount DOUBLE,\n",
    "      tolls_amount DOUBLE,\n",
    "      improvement_surcharge DOUBLE,\n",
    "      total_amount DOUBLE,\n",
    "      congestion_surcharge DOUBLE,\n",
    "      Airport_fee DOUBLE,\n",
    "      month INT,\n",
    "      year INT)\n",
    "      USING iceberg\n",
    "      PARTITIONED BY (year, month)\n",
    "      LOCATION '<warehouse_path>/warehouse/nyc_tlc/yellow_taxi_trips'\n",
    "      TBLPROPERTIES (\n",
    "        'current-snapshot-id' = '6410054250659262609',\n",
    "        'format' = 'iceberg/parquet',\n",
    "        'format-version' = '2',\n",
    "        'write.parquet.compression-codec' = 'zstd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name|type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main|BRANCH|6410054250659262609|NULL                   |NULL                 |NULL                  |\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# metdata tables: `refs`, `history`, `manifests`, `partitions`, `snapshots`, `files`\n",
    "spark.sql(\"select * from local.nyc_tlc.yellow_taxi_trips.refs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WAP implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_table = \"local.nyc_tlc.yellow_taxi_trips\"\n",
    "audit_branch = f\"audit_102023\"\n",
    "\n",
    "# Create an Audit Branch for staging the new data before writing in prod table\n",
    "spark.sql(f\"ALTER TABLE {prod_table} CREATE BRANCH {audit_branch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name        |type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main        |BRANCH|6410054250659262609|NULL                   |NULL                 |NULL                  |\n",
      "|audit_102023|BRANCH|6410054250659262609|NULL                   |NULL                 |NULL                  |\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking if a new audit branch is created from same snapshot as main branch\n",
    "spark.sql(\"select * from local.nyc_tlc.yellow_taxi_trips.refs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/25 18:37:33 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the table has WAP enabled, if not enable wap \n",
    "spark.sql(f\"ALTER TABLE {prod_table} SET TBLPROPERTIES ('write.wap.enabled'='true')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CREATE TABLE local.nyc_tlc.yellow_taxi_trips (\\n  VendorID INT,\\n  tpep_pickup_datetime TIMESTAMP_NTZ,\\n  tpep_dropoff_datetime TIMESTAMP_NTZ,\\n  passenger_count BIGINT,\\n  trip_distance DOUBLE,\\n  RatecodeID BIGINT,\\n  store_and_fwd_flag STRING,\\n  PULocationID INT,\\n  DOLocationID INT,\\n  payment_type BIGINT,\\n  fare_amount DOUBLE,\\n  extra DOUBLE,\\n  mta_tax DOUBLE,\\n  tip_amount DOUBLE,\\n  tolls_amount DOUBLE,\\n  improvement_surcharge DOUBLE,\\n  total_amount DOUBLE,\\n  congestion_surcharge DOUBLE,\\n  Airport_fee DOUBLE,\\n  month INT,\\n  year INT)\\nUSING iceberg\\nPARTITIONED BY (year, month)\\nLOCATION '/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/yellow_taxi_trips'\\nTBLPROPERTIES (\\n  'current-snapshot-id' = '6410054250659262609',\\n  'format' = 'iceberg/parquet',\\n  'format-version' = '2',\\n  'write.parquet.compression-codec' = 'zstd',\\n  'write.wap.enabled' = 'true')\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking if wap is enabled in table properties\n",
    "spark.sql(\"show create table local.nyc_tlc.yellow_taxi_trips\").toPandas().to_dict(orient=\"records\")[0]['createtab_stmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting WAP branch to Audit Branch in Spark Session so new data is written into Audit Branch\n",
    "spark.conf.set(\"spark.wap.branch\", audit_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3522285"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading new Oct 2023 Yellow Taxi trip data that needs to be written into Audit Branch.\n",
    "new_data = spark.read.parquet(\"../nyc-taxi-trips/yellow/oct-2023/\")\n",
    "new_data = new_data.withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(10))\n",
    "new_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## Writing new data into Audit branch of table\n",
    "new_data.writeTo(prod_table).append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|year|month|  count|\n",
      "+----+-----+-------+\n",
      "|2023|   10|3522285|\n",
      "|2023|    9|2846722|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check if the main production table data is not hampered\n",
    "main_df = spark.table(\"local.nyc_tlc.yellow_taxi_trips\")\n",
    "main_df.groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila..!!! our main table data is impacted after write?!?!?!\n",
    "\n",
    "Not Really. As we have set `spark.wap.branch` to `audit_branch` when we are reading from table it's gonna read from the same branch.\n",
    "\n",
    "It's equivalent to reading from `audit_branch` i.e. `spark.read.option(\"BRANCH\", \"audit_102023\")` or,\n",
    "\n",
    "in sql terms `SELECT * FROM local.nyc_tlc.yellow_taxi_trips VERSION AS OF 'audit_102023';`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|year|month|  count|\n",
      "+----+-----+-------+\n",
      "|2023|    9|2846722|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Reading from `main` production branch \"main\"\n",
    "spark.read.option(\"BRANCH\",\"main\").table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in our prod table `main` branch is intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing data present in Audit Branch.\n",
    "\n",
    "Let's assume as per our application data quality standards:\n",
    "- It shouldn't have any data with `total_amount` as negative.\n",
    "- The completeness of VendorID should be 1.0 i.e. there shouldn't be any `null` in `VendorID` field.\n",
    "- `payment_type` should contain only discrete numeric values from 1 to 6\n",
    "\n",
    "So these are mainly nothing but data quality rules that you can run on the `audit branch` and decide what you want to do with the data for e.g.\n",
    "\n",
    "- Discard the entire snapshot as the DQ doesn't meet the expectation.\n",
    "- DELETE the rows not meeting DQ standards and preserve such records somewhere else.\n",
    "- Fix the data via some logic like populating `null` and `missing` value with some logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# Running DQ checks as per the requirements using spark itself, \n",
    "# It can be anything here: like PyDeequ Verifications, Glue Data Quality Job Runs here.\n",
    "\n",
    "# Reading data from Audit Branch:\n",
    "audit_data = spark.read.option(\"BRANCH\", audit_branch).table(prod_table)\n",
    "\n",
    "# check if there are any rows with negative total_amount\n",
    "neg_amt_df = audit_data.filter(col(\"total_amount\") < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if not neg_amt_df.isEmpty():\n",
    "    # write rejected records in some table or someplace before deleting them.\n",
    "    # neg_amt_df.write.partitionedBy(\"year\",\"month\").parquet(\"bad-data-location\")\n",
    "    spark.sql(f\"DELETE FROM {prod_table} where total_amount < 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2023-12-25 17:37:57.422|6410054250659262609|NULL     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.nyc_tlc.yellow_taxi_trips.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|year|month|  count|\n",
      "+----+-----+-------+\n",
      "|2023|    9|2846722|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data in main branch\n",
    "spark.read.option(\"BRANCH\",\"main\").table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+\n",
      "|year|month|  count|\n",
      "+----+-----+-------+\n",
      "|2023|    9|2817469|\n",
      "|2023|   10|3485320|\n",
      "+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data in audit branch -- changes after few records are deleted as per Auditing\n",
    "spark.read.option(\"BRANCH\", audit_branch).table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Auditing is done and DQ is as expected or fixed. We can Publish these final records into the Production main branch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Fast forwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2023-12-25 17:37:57.422|6410054250659262609|NULL     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Before Fast forwarding\n",
    "spark.sql(f\"select * from {prod_table}.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name        |type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main        |BRANCH|6410054250659262609|NULL                   |NULL                 |NULL                  |\n",
      "|audit_102023|BRANCH|3963454346673756003|NULL                   |NULL                 |NULL                  |\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {prod_table}.refs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Forwarding audit branch to main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+\n",
      "|branch_updated|       previous_ref|        updated_ref|\n",
      "+--------------+-------------------+-------------------+\n",
      "|          main|6410054250659262609|3963454346673756003|\n",
      "+--------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/25 20:23:11 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"CALL local.system.fast_forward('{prod_table}', 'main', '{audit_branch}')\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2023-12-25 17:37:57.422|6410054250659262609|NULL               |true               |\n",
      "|2023-12-25 20:23:10.974|3963454346673756003|9062254653834310700|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {prod_table}.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name        |type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main        |BRANCH|3963454346673756003|NULL                   |NULL                 |NULL                  |\n",
      "|audit_102023|BRANCH|3963454346673756003|NULL                   |NULL                 |NULL                  |\n",
      "+------------+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select * from {prod_table}.refs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsetting wap.branch from the spark session\n",
    "spark.conf.unset('spark.wap.branch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
