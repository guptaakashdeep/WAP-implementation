{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAP implementation for Iceberg version < 1.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark=3.3\n",
    "!pip install findspark\n",
    "!pip install pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Iceberg jars \n",
    "- .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2')\n",
    "\n",
    "Loading Iceberg Extensions to call stored procedures\n",
    "- .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\n",
    "\n",
    "Creating Iceberg catalog name of type `hive` that loads table from `Hive Metastore`. Adds support for spark built-in catalog \n",
    "- .config('spark.sql.catalog.spark_catalog','org.apache.iceberg.spark.SparkSessionCatalog')\n",
    "- .config('spark.sql.catalog.spark_catalog.type','hive')\n",
    "\n",
    "Creating Iceberg catalog named `local` of type `hadoop`. This supports directory based catalog in HDFS\n",
    "- .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog')\n",
    "- .config('spark.sql.catalog.local.type','hadoop')\n",
    "- .config('spark.sql.catalog.local.warehouse','<path_to_warehouse>') \\\n",
    "\n",
    "If `type` is `null`, `spark.sql.catalog.<catalog-name>.catalog-impl` **shouldn't** be `null`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_VERSION\"] = '3.3'\n",
    "import pydeequ\n",
    "\n",
    "warehouse_directory = \"local_path\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"wap-iceberg-1.2.0\") \\\n",
    "    .config('spark.jars.packages', f'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.2.0,{pydeequ.deequ_maven_coord}')\\\n",
    "    .config('spark.jars.excludes', pydeequ.f2j_maven_coord) \\\n",
    "    .config('spark.sql.extensions','org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions') \\\n",
    "    .config('spark.sql.catalog.spark_catalog','org.apache.iceberg.spark.SparkSessionCatalog') \\\n",
    "    .config('spark.sql.catalog.spark_catalog.type','hive') \\\n",
    "    .config('spark.sql.catalog.local','org.apache.iceberg.spark.SparkCatalog') \\\n",
    "    .config('spark.sql.catalog.local.type','hadoop') \\\n",
    "    .config('spark.sql.catalog.local.warehouse',f'{warehouse_directory}/warehouse') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Creating Iceberg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: long (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 65471)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_df = spark.read.parquet(\"../nyc-taxi-trips/green/sep-2023/\")\n",
    "green_df.printSchema(), green_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating month and year column\n",
    "from pyspark.sql.functions import lit\n",
    "green_df = green_df.withColumn(\"month\", lit(9)) \\\n",
    "        .withColumn(\"year\", lit(2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and Iceberg table\n",
    "green_df.writeTo(\"local.nyc_tlc.green_taxi_trips\").partitionedBy(\"year\", \"month\").using(\"iceberg\") \\\n",
    "    .tableProperty(\"format-version\", \"2\") \\\n",
    "    .tableProperty(\"write.parquet.compression-codec\", \"snappy\") \\\n",
    "    .create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_stmt = spark.sql(\"show create table local.nyc_tlc.green_taxi_trips\").collect()[0]['createtab_stmt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE Iceberg local.nyc_tlc.green_taxi_trips (\n",
    "    VendorID INT,\n",
    "    lpep_pickup_datetime TIMESTAMP,\n",
    "    lpep_dropoff_datetime TIMESTAMP,\n",
    "    store_and_fwd_flag STRING,\n",
    "    RatecodeID BIGINT,\n",
    "    PULocationID INT,\n",
    "    DOLocationID INT,\n",
    "    passenger_count BIGINT,\n",
    "    trip_distance DOUBLE,\n",
    "    fare_amount DOUBLE,\n",
    "    extra DOUBLE,\n",
    "    mta_tax DOUBLE,\n",
    "    tip_amount DOUBLE,\n",
    "    tolls_amount DOUBLE,\n",
    "    ehail_fee DOUBLE,\n",
    "    improvement_surcharge DOUBLE,\n",
    "    total_amount DOUBLE,\n",
    "    payment_type BIGINT,\n",
    "    trip_type BIGINT,\n",
    "    congestion_surcharge DOUBLE,\n",
    "    month INT,\n",
    "    year INT)\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (year, month)\n",
    "    LOCATION '<warehouse_path>/nyc_tlc/green_taxi_trips'\n",
    "    TBLPROPERTIES (\n",
    "        'current-snapshot-id' = '9122000941857891650',\n",
    "        'format' = 'iceberg/parquet',\n",
    "        'format-version' = '2',\n",
    "        'write.parquet.compression-codec' = 'snappy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                                                                                                    |summary                                                                                                                                                                                                                                                                                                       |\n",
      "+-----------------------+-------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2023-12-26 18:02:23.222|9122000941857891650|null     |append   |/Users/akashdeepgupta/Documents/project-repos/pyspark-playground/warehouse/nyc_tlc/green_taxi_trips/metadata/snap-9122000941857891650-1-87318e3f-7c45-4c11-96d7-fa7075bc05c3.avro|{spark.app.id -> local-1703577889828, added-data-files -> 1, added-records -> 65471, added-files-size -> 1514631, changed-partition-count -> 1, total-records -> 65471, total-files-size -> 1514631, total-data-files -> 1, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from local.nyc_tlc.green_taxi_trips.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new data into production table using WAP enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66177"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read new data that needs to be written in to prod table\n",
    "new_df = spark.read.parquet(\"../nyc-taxi-trips/green/oct-2023/\")\n",
    "new_df = new_df.withColumn(\"year\", lit(2023)).withColumn(\"month\", lit(10))\n",
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/26 18:02:40 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod_table = \"local.nyc_tlc.green_taxi_trips\"\n",
    "\n",
    "## Check and add write.wap.enabled = true in prod table properties if not present.\n",
    "spark.sql(f\"ALTER TABLE {prod_table} SET TBLPROPERTIES ('write.wap.enabled' = 'true')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tblproperties are updated with write.wap.enabled = true\n",
    "spark.sql(\"show create table local.nyc_tlc.green_taxi_trips\").collect()[0]['createtab_stmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de6d592742b54cf09353fb168d6dc856\n"
     ]
    }
   ],
   "source": [
    "# As Branching is not available in Iceberg < 1.2.1, \n",
    "# generated a wap.id that will be used to identify the snapshot_id for new data written into table\n",
    "\n",
    "import uuid\n",
    "wap_id = uuid.uuid4().hex\n",
    "\n",
    "# set wap.id in spark session to put it in the summary of snapshot\n",
    "spark.conf.set(\"spark.wap.id\", wap_id)\n",
    "\n",
    "print(wap_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new data into table now\n",
    "new_df.writeTo(prod_table).append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2023|    9|65471|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the snapshots for the table\n",
    "spark.sql(f\"select * from {prod_table}.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audit\n",
    "\n",
    "- Get snapshot_id corresponding to the wap.id that was generated.\n",
    "- Read data based on this snapshot_id.\n",
    "- Run your DQ tests on it.\n",
    "- Based on DQ Test results, you can choose if this new snapshot needs to be published further or can be expired or deleted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/26 18:03:06 WARN SparkScanBuilder: Failed to check if IsNotNull(summary) can be pushed down: Cannot find field 'summary' in struct: struct<>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3620034948650558114"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "staged_data_snap_id = spark.sql(f\"select snapshot_id, summary['wap.id'] as wap_id from {prod_table}.snapshots\").filter(col(\"wap_id\") == wap_id).collect()[0]['snapshot_id']\n",
    "staged_data_snap_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the staged data from prod_table\n",
    "staged_data = spark.read.option(\"snapshot-id\", staged_data_snap_id).table(prod_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2023|    9|65471|\n",
      "|2023|   10|66177|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "staged_data.groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running DQ tests using PyDeequ.\n",
    "\n",
    "Let's say your DQ requirement is:\n",
    "- Completeness criteria for `VendorID` should be 1.0 i.e. there are no `null`.\n",
    "- `total_amount` shouldn't be `negative`\n",
    "- `payment_type` should be discrete numbers among 1 to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Run Status: Warning\n",
      "+-------------------+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "|check              |check_level|check_status|constraint                                                                                                                                        |constraint_status|constraint_message                                                 |\n",
      "+-------------------+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "|VendorID Checks    |Error      |Success     |CompletenessConstraint(Completeness(VendorID,None))                                                                                               |Success          |                                                                   |\n",
      "|payment_type Checks|Error      |Success     |ComplianceConstraint(Compliance(payment_type contained in 1,2,3,4,5,6,`payment_type` IS NULL OR `payment_type` IN ('1','2','3','4','5','6'),None))|Success          |                                                                   |\n",
      "|total_amount Checks|Warning    |Warning     |ComplianceConstraint(Compliance(total_amount is non-negative,COALESCE(CAST(total_amount AS DECIMAL(20,10)), 0.0) >= 0,None))                      |Failure          |Value: 0.9968780384054449 does not meet the constraint requirement!|\n",
      "+-------------------+-----------+------------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashdeepgupta/Documents/project-repos/git-repos/aws-rss-discord-bot/venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "# Check represents a list of constraints that can be applied to a provided Spark Dataframe\n",
    "vendorID_checks = Check(spark, CheckLevel.Error, \"VendorID Checks\")\n",
    "payment_type_checks = Check(spark, CheckLevel.Error, \"payment_type Checks\")\n",
    "tot_amt_checks = Check(spark, CheckLevel.Warning, \"total_amount Checks\")\n",
    "\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(staged_data) \\\n",
    "    .addCheck(vendorID_checks.isComplete(\"VendorID\")) \\\n",
    "    .addCheck(payment_type_checks.isContainedIn(\"payment_type\", ['1','2','3','4','5','6'])) \\\n",
    "    .addCheck(tot_amt_checks.isNonNegative(\"total_amount\")) \\\n",
    "    .run()\n",
    "\n",
    "print(f\"Verification Run Status: {checkResult.status}\")\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2023|    9|65263|\n",
      "|2023|   10|65974|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(f\"DELETE FROM {prod_table} where total_amount < 0\")\n",
    "cleaned_df = staged_data.filter(col(\"total_amount\") >= 0)\n",
    "# write the cleaned_df to the prod_table, this will be written with the same wap.id that we have set in SparkSession\n",
    "# so it will still be like writing data into prod table from start. Check parent_id after writing it should be same for both the commits.\n",
    "#cleaned_df.writeTo(prod_table).append()\n",
    "cleaned_df.groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing cleaned data \n",
    "cleaned_df.writeTo(prod_table).overwritePartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/26 17:26:47 WARN SparkScanBuilder: Failed to check if IsNotNull(summary) can be pushed down: Cannot find field 'summary' in struct: struct<>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7581908246933781414"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the latest snapshot_id after the cleaned data is written\n",
    "udpated_data_snap_id = spark.sql(f\"select snapshot_id, summary['wap.id'] as wap_id from {prod_table}.snapshots\").filter(col(\"wap_id\") == wap_id).orderBy(col(\"committed_at\").desc()).collect()[0]['snapshot_id']\n",
    "udpated_data_snap_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2023|    9|65471|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking in data in Production Table before Publishing\n",
    "spark.table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/26 17:26:56 WARN BaseTransaction: Failed to load metadata for a committed snapshot, skipping clean-up\n",
      "+-------------------+-------------------+\n",
      "| source_snapshot_id|current_snapshot_id|\n",
      "+-------------------+-------------------+\n",
      "|7581908246933781414|7581908246933781414|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# call cherrypick to publish the updated final snapshot with clean data in Production Table\n",
    "spark.sql(f\"CALL local.system.cherrypick_snapshot('{prod_table}', {udpated_data_snap_id})\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|year|month|count|\n",
      "+----+-----+-----+\n",
      "|2023|    9|65263|\n",
      "|2023|   10|65974|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifying data after Publish\n",
    "spark.table(prod_table).groupBy(\"year\",\"month\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validating data if there are any records with negative total_amount\n",
    "spark.sql(f\"select count(*) from {prod_table} where total_amount < 0\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
